{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tensorflow version 확인\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "초기화 : 신경망의 초깃값을 지정해주는 것\n",
    "- 난수로 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.12675226], dtype=float32)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3.7 균일분포로부터 난수 얻기\n",
    "rand = tf.random.uniform([1], 0, 1)\n",
    "rand\n",
    "# 출력값 : <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.40041852], dtype=float32)>\n",
    "# Tensor 라는 객체 반환\n",
    "# Tensor 참조 문서 : https://www.tensorflow.org/api_docs/python/tf/Tensor\n",
    "# shape : Numpy 배열의 크기\n",
    "# tensorflow의 data type 참조 문서 : https://www.tensorflow.org/api_docs/python/tf/dtypes/DType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0.41025015], shape=(1,), dtype=float64)\n",
      "tf.Tensor([0.39312406 0.48611453 0.94098132 0.29909521], shape=(4,), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0.07979111 0.23110839]\n",
      " [0.20871277 0.22348299]], shape=(2, 2), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "rand2 = tf.random.uniform([1], 0, 1, dtype=tf.dtypes.float64)  # 균일분포\n",
    "                                                               # dtype 지정 가능\n",
    "print(rand2)\n",
    "rand3 = tf.random.uniform([4], 0, 1, dtype=tf.dtypes.float64)\n",
    "print(rand3)\n",
    "rand4 = tf.random.uniform([2,2], 0, 1, dtype=tf.dtypes.float64)\n",
    "print(rand4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,)\n",
      "<dtype: 'float64'>\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(rand2.shape)  # shape\n",
    "print(rand2.dtype)  # data type\n",
    "print(rand2.ndim)   # 차원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[-0.3142903 ,  1.5370331 ],\n",
       "       [ 0.31200996, -1.0209621 ]], dtype=float32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정규분포로부터 난수 추출\n",
    "rand = tf.random.normal([2,2], 0, 1)\n",
    "rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=float32, numpy=array([12.294958 ,  7.4376955, 10.072827 , 13.40919  ], dtype=float32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand2 = tf.random.normal([4], 10, 2)\n",
    "rand2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 활성화함수 sigmoid 정의\n",
    "import math\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5231249861119812\n",
      "-0.5231249861119812\n"
     ]
    }
   ],
   "source": [
    "# 3.11 뉴런의 입력과 출력 정의\n",
    "x = 1  # 입력\n",
    "y = 0  # 기대출력\n",
    "w = tf.random.normal([1], 0, 1)  # 입력의 가중치\n",
    "output = sigmoid(x * w)\n",
    "print(output)  # 출력\n",
    "print(y - output)  # 오류 = 기대출력 - 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99번째 학습, 오차: -0.1021526091193261, 출력값: 0.1021526091193261\n",
      "199번째 학습, 오차: -0.05235501578907714, 출력값: 0.05235501578907714\n",
      "299번째 학습, 오차: -0.03484877242329319, 출력값: 0.03484877242329319\n",
      "399번째 학습, 오차: -0.026035323509845554, 출력값: 0.026035323509845554\n",
      "499번째 학습, 오차: -0.020751866648527167, 출력값: 0.020751866648527167\n",
      "599번째 학습, 오차: -0.017238778301018963, 출력값: 0.017238778301018963\n",
      "699번째 학습, 오차: -0.014736815953837275, 출력값: 0.014736815953837275\n",
      "799번째 학습, 오차: -0.012865637046209029, 출력값: 0.012865637046209029\n",
      "899번째 학습, 오차: -0.011414055227549081, 출력값: 0.011414055227549081\n",
      "999번째 학습, 오차: -0.010255516060579154, 출력값: 0.010255516060579154\n"
     ]
    }
   ],
   "source": [
    "# 3.12 경사하강법을 이용한 뉴런의 학습\n",
    "x = 1\n",
    "y = 0\n",
    "w = tf.random.normal([1], 0, 1)\n",
    "\n",
    "for i in range(1000):\n",
    "    output = sigmoid(x * w)\n",
    "    error = y - output  # 오류 = 기대출력 - 출력\n",
    "    w = w + x * 0.1 * error  # 학습률=0.1 : 가중치(w)를 업데이트하는 정도\n",
    "    \n",
    "    if i % 100 == 99:\n",
    "        print(f\"{i}번째 학습, 오차: {error}, 출력값: {output}\")\n",
    "# 0.12에서 0.01까지 내려감\n",
    "# 예측출력 0에 가까워짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99번째 학습, 오차: 0.5, 출력값: 0.5\n",
      "199번째 학습, 오차: 0.5, 출력값: 0.5\n",
      "299번째 학습, 오차: 0.5, 출력값: 0.5\n",
      "399번째 학습, 오차: 0.5, 출력값: 0.5\n",
      "499번째 학습, 오차: 0.5, 출력값: 0.5\n",
      "599번째 학습, 오차: 0.5, 출력값: 0.5\n",
      "699번째 학습, 오차: 0.5, 출력값: 0.5\n",
      "799번째 학습, 오차: 0.5, 출력값: 0.5\n",
      "899번째 학습, 오차: 0.5, 출력값: 0.5\n",
      "999번째 학습, 오차: 0.5, 출력값: 0.5\n"
     ]
    }
   ],
   "source": [
    "# 3.13 x=0, y=1을 얻는 뉴런의 학습\n",
    "x = 0\n",
    "y = 1\n",
    "w = tf.random.normal([1], 0, 1)\n",
    "\n",
    "for i in range(1000):\n",
    "    output = sigmoid(x * w)\n",
    "    error = y - output\n",
    "    w = w + x * 0.1 * error\n",
    "    \n",
    "    if i % 100 == 99:\n",
    "        print(f\"{i}번째 학습, 오차: {error}, 출력값: {output}\")\n",
    "\n",
    "# 오차, 출력값 변하지 않음\n",
    "# 이유? 입력값(x)=0 > w 변화 없음\n",
    "\n",
    "# 이러한 상황을 방지하기 위해 편향(bias)를 뉴런에 넣어주자\n",
    "# 편향의 입력 : 보편적으로 1 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 0.10754276066612434 0.8924572393338757\n",
      "199 0.05381407588770648 0.9461859241122935\n",
      "299 0.035501047698687116 0.9644989523013129\n",
      "399 0.02640092740736999 0.97359907259263\n",
      "499 0.020984690636869363 0.9790153093631306\n",
      "599 0.017399706100334034 0.982600293899666\n",
      "699 0.014854540087022583 0.9851454599129774\n",
      "799 0.01295541442201964 0.9870445855779804\n",
      "899 0.01148479351980125 0.9885152064801987\n",
      "999 0.010312664758291623 0.9896873352417084\n"
     ]
    }
   ],
   "source": [
    "# 3.14. x = 0, y = 0, 편향 추가\n",
    "x = 0\n",
    "y = 1\n",
    "w = tf.random.normal([1], 0, 1)\n",
    "b = tf.random.normal([1], 0, 1)  # bias(편향) 추가\n",
    "\n",
    "for i in range(1000):\n",
    "    output = sigmoid(x * w + 1 * b)  # 편향의 입력으로는 보통 1 사용\n",
    "    error = y - output\n",
    "    w = w + x * 0.1 * error\n",
    "    b = b + 1 * 0.1 * error\n",
    "    \n",
    "    if i % 100 == 99:\n",
    "        print(i, error, output)\n",
    "# 0.89에서 0.99까지 올라감\n",
    "# 예측출력 1에 가까워짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199 -0.11423061914864407\n",
      "399 -0.06706393378144553\n",
      "599 -0.047367154376369486\n",
      "799 -0.036505987719282894\n",
      "999 -0.029640264892567884\n",
      "1199 -0.024919307073971157\n",
      "1399 -0.021478692059489525\n",
      "1599 -0.018863403586778023\n",
      "1799 -0.01680807214131866\n",
      "1999 -0.015153544743112084\n"
     ]
    }
   ],
   "source": [
    "# 3.16 첫 번째 신경망 네트워크 : AND\n",
    "import numpy as np\n",
    "x = np.array([   # 입력값\n",
    "    [1, 1],\n",
    "    [1, 0],\n",
    "    [0, 1],\n",
    "    [0, 0]\n",
    "])\n",
    "y = np.array([   # 기대출력\n",
    "    [1],\n",
    "    [0],\n",
    "    [0],\n",
    "    [0] \n",
    "])\n",
    "w = tf.random.normal([2], 0, 1)  # 입력값의 가중치\n",
    "                                 # 입력1, 입력2 -> 2개니까 가중치도 각각해서 총 2개\n",
    "b = tf.random.normal([1], 0, 1)  # 편향(bias)\n",
    "b_x = 1                          # 편향의 입력값\n",
    "alpha = 0.1                      # 학습률\n",
    "\n",
    "for i in range(2000):  # 학습수 : 2000번\n",
    "    error_sum = 0\n",
    "    for j in range(4): # 한 번 학습할 때마다 데이터 4개를 입력하니까\n",
    "        output =  sigmoid(np.sum(x[j] * w) + b_x * b)  # 입력값 * 가중치를 다 더함\n",
    "                                                       # 데이터 4개 + 편향 전체의 (입력값 * 가중치)의 합\n",
    "                                                       # 참고) numpy의 ndarray는 *할 때 shape에 맞게 알아서 조절이 된다.\n",
    "        error = y[j][0] - output  # 오차값 = 기대출력 - 출력\n",
    "        \n",
    "        # 경사하강법에 의한 가중치, 편향의 조정\n",
    "        w = w + x[j] * alpha * error\n",
    "        b = b + b_x * alpha * error\n",
    "        error_sum += error                             # 데이터별로 error 나오니까 데이터 4개의 에러총 합이 학습 1번할때마다의 오류\n",
    "    if i % 200 == 199:\n",
    "        print(i, error_sum)\n",
    "# print(x[0])  # [1 1]\n",
    "# print(x[1])  # [1 0]\n",
    "# print(x[2])  # [0 1]\n",
    "# print(x[3])  # [0 0]\n",
    "# print(w)  # 만약 [1.02~ 0.85~] 라면\n",
    "# print(x[0] * w)  # [1 1] * [1.02~ 0.85~] = [1.02~ 0.85~]\n",
    "# print(x[1] * w)  # [1 0] * [1.02~ 0.85~] = [1.02~ 0.]\n",
    "# print(x[0] * w + x[1] * w + x[2] * w + x[3] * w + b_x * b)  # 각각의 입력값 * 가중치를 다 더한게 output. 출력값."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: [1 1] Y: [1] Output: 0.9646230992248306\n",
      "X: [1 0] Y: [0] Output: 0.02506809154703148\n",
      "X: [0 1] Y: [0] Output: 0.025145249934152486\n",
      "X: [0 0] Y: [0] Output: 2.432284585414779e-05\n"
     ]
    }
   ],
   "source": [
    "# AND 신경망 네트워크 평가\n",
    "# 학습 2000번 시킨 후의 결과로 평가\n",
    "# 학습시킨 네트워크가 정상적으로 작동하는지 평가\n",
    "# 네트워크에 각각의 입력값(x)들을 넣었을 때 실제출력(output)이 기대출력(y)값에 얼마나 가까운지 확인\n",
    "for i in range(4):\n",
    "    print(\"X:\", x[i], \"Y:\", y[i], \"Output:\", sigmoid(np.sum(x[i] * w) + b_x * b))\n",
    "# 출력값을 보면 각각의 기대출력인 1, 0, 0, 0값에 가깝게 나온다는 것을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199 -0.04520820735766512\n",
      "399 -0.02448035983431883\n",
      "599 -0.016732240063228113\n",
      "799 -0.012673551438553199\n",
      "999 -0.010183065790098991\n",
      "1199 -0.008502391983647745\n",
      "1399 -0.007293464875306015\n",
      "1599 -0.006382674474522537\n",
      "1799 -0.005673121868035008\n",
      "1999 -0.005103187908561874\n"
     ]
    }
   ],
   "source": [
    "# 3.21 두 번째 신경망 네트워크 : OR\n",
    "# AND 네트워크와의 차이점 : 기대출력(y)\n",
    "x = np.array([\n",
    "    [1, 1],\n",
    "    [1, 0],\n",
    "    [0, 1],\n",
    "    [0, 0]\n",
    "])\n",
    "y = np.array([\n",
    "    [1],\n",
    "    [1],\n",
    "    [1],\n",
    "    [0]\n",
    "])\n",
    "w = tf.random.normal([2], 0, 1)\n",
    "b = tf.random.normal([1], 0, 1)\n",
    "b_x = 1\n",
    "alpha = 0.1\n",
    "\n",
    "for i in range(2000):\n",
    "    error_sum = 0\n",
    "    for j in range(4):\n",
    "        output = sigmoid(np.sum(x[j] * w) + b_x * b)\n",
    "        error = y[j][0] - output\n",
    "        \n",
    "        # 경사하강법에 의한 가중치, 편향의 조정\n",
    "        w = w + x[j] * alpha * error\n",
    "        b = b + b_x * alpha * error\n",
    "        \n",
    "        error_sum += error\n",
    "    if i % 200 == 199:\n",
    "        print(i, error_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: [1 1] Y: [1] Output: 0.9999972808626955\n",
      "X: [1 0] Y: [1] Output: 0.9898627291943352\n",
      "X: [0 1] Y: [1] Output: 0.9898762230752566\n",
      "X: [0 0] Y: [0] Output: 0.025304230326331455\n"
     ]
    }
   ],
   "source": [
    "# OR 신경망 네트워크 평가\n",
    "# 학습 2000번 시킨 후의 결과로 평가\n",
    "for i in range(4):\n",
    "    print(\"X:\", x[i], \"Y:\", y[i], \"Output:\", sigmoid(np.sum(x[i] * w) + b_x * b))\n",
    "# 출력값을 보면 각각의 기대출력인 1, 1, 1, 0에 가까운 실제출력값이 나온다는 것 확인 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199 -0.0004907606742271531\n",
      "399 -1.9945055159897862e-05\n",
      "599 -8.059952041339358e-07\n",
      "799 -6.514973560634019e-09\n",
      "999 3.722842145670313e-09\n",
      "1199 3.722842145670313e-09\n",
      "1399 3.722842145670313e-09\n",
      "1599 3.722842145670313e-09\n",
      "1799 3.722842145670313e-09\n",
      "1999 3.722842145670313e-09\n"
     ]
    }
   ],
   "source": [
    "# 3.23 세 번째 신경망 네트워크 : XOR\n",
    "# XOR : 입력값들 중 참이 홀수개일 때만 -> 결과값 : 참\n",
    "# AND, OR 신경망과의 차이점 : 기대출력(y)\n",
    "x = np.array([\n",
    "    [1, 1],\n",
    "    [1, 0],\n",
    "    [0, 1],\n",
    "    [0, 0]\n",
    "])\n",
    "y = np.array([\n",
    "    [0],\n",
    "    [1],\n",
    "    [1],\n",
    "    [0]\n",
    "])\n",
    "w = tf.random.normal([2], 0, 1)\n",
    "b = tf.random.normal([1], 0, 1)\n",
    "b_x = 1\n",
    "alpha = 0.1\n",
    "\n",
    "for i in range(2000):\n",
    "    error_sum = 0\n",
    "    for j in range(4):\n",
    "        output = sigmoid(np.sum(x[j] * w) + b_x * b)\n",
    "        error = y[j][0] - output\n",
    "        \n",
    "        w = w + x[j] * alpha * error\n",
    "        b = b + b_x * alpha * error\n",
    "        \n",
    "        error_sum += error\n",
    "    \n",
    "    if i % 200 == 199:\n",
    "        print(i, error_sum)\n",
    "\n",
    "# AND, OR 신경망 네트워크와는 다르게\n",
    "# 어느순간 error_sum 이 변화하지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: [1 1] Y: [0] Output: 0.5128176286712095\n",
      "X: [1 0] Y: [1] Output: 0.5128176305326305\n",
      "X: [0 1] Y: [1] Output: 0.4999999990686774\n",
      "X: [0 0] Y: [0] Output: 0.5000000009313226\n"
     ]
    }
   ],
   "source": [
    "# 3.24 XOR 신경망 네트워크 평가\n",
    "# 학습 2000번 시킨 후의 결과로 평가\n",
    "for i in range(4):\n",
    "    print(\"X:\", x[i], \"Y:\", y[i], \"Output:\", sigmoid(np.sum(x[i] * w) + b_x * b))\n",
    "# 출력값을 보면 각각의 기대출력인 0, 1, 1, 0과 차이가 좀 있음..\n",
    "# 입력값(x)이 변화하였는데도\n",
    "# 실제출력값(output)은 0.5부근의 값을 보인다.\n",
    "# 그 이유에 대해 알아보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: tf.Tensor([ 5.1281754e-02 -7.4505806e-09], shape=(2,), dtype=float32) b: tf.Tensor([3.7252903e-09], shape=(1,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 3.25 XOR 신경망 네트워크의 가중치들(w, b) 확인\n",
    "print(\"w:\", w, \"b:\", b)\n",
    "# 출력값 :\n",
    "# w: tf.Tensor([5.1281769e-02 3.7252903e-09], shape=(2,), dtype=float32) b: tf.Tensor([-7.450581e-09], shape=(1,), dtype=float32)\n",
    "# 가중치1 : 0.05\n",
    "# 가중치2 : 0.000000003\n",
    "# 편향 : -0.00000007\n",
    "# 입력1이 입력2보다 큰 영향을 끼친다.\n",
    "# 가중치1이 제일 영향이 크다.\n",
    "# 가중치2, 편향은 거의 영향이 없다.\n",
    "# 따라서 기대출력(y)과 실제출력(output) 사이의 차이가 크게 나오는 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: [1 1] y: [0] 중간계산값: tf.Tensor([0.05128175], shape=(1,), dtype=float32) 출력값(sigmoid(중간계산값)): 0.5128176286712095\n",
      "X: [1 0] y: [1] 중간계산값: tf.Tensor([0.05128176], shape=(1,), dtype=float32) 출력값(sigmoid(중간계산값)): 0.5128176305326305\n",
      "X: [0 1] y: [1] 중간계산값: tf.Tensor([-3.7252903e-09], shape=(1,), dtype=float32) 출력값(sigmoid(중간계산값)): 0.4999999990686774\n",
      "X: [0 0] y: [0] 중간계산값: tf.Tensor([3.7252903e-09], shape=(1,), dtype=float32) 출력값(sigmoid(중간계산값)): 0.5000000009313226\n"
     ]
    }
   ],
   "source": [
    "# sigmoid 활성화 함수에 넣기 전, 중간계산값(입력값*가중치의 총합)과\n",
    "# sigmoid 활성화 함수에 넣은 후 비교\n",
    "for i in range(4):\n",
    "    print(\"X:\", x[i], \"y:\", y[i], \"중간계산값:\", np.sum(x[i]*w) + b_x * b, \"출력값(sigmoid(중간계산값)):\", sigmoid(np.sum(x[i]*w) + b_x * b))\n",
    "    \n",
    "# sigmoid 활성화 함수에 넣기전, 중간계산값이\n",
    "# 점점 0에 가까워진다.\n",
    "# 앞에서 0에 가까워지게 치우쳐지면 \n",
    "\n",
    "# x = 0 -> w값 변화 없음\n",
    "# 뉴런이 학습을 하지 못함.\n",
    "# 한쪽으로 치우쳐진 상황임\n",
    "\n",
    "# 이런 상황인것을 봤었다.\n",
    "# 따라서 입력1에 따라 중간계산값에는 차이가 발생할 수 있지만\n",
    "# 출력값(y)에는 별 차이가 없어진 것을 알 수 있다.\n",
    "\n",
    "# 이렇게 가중치, 편향이 미치는 영향에 대해 더 자세히 알아보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: tf.Tensor([6.993875  6.9969835], shape=(2,), dtype=float32) b: tf.Tensor([-10.669766], shape=(1,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 앞의 AND 신경망 네트워크의 가중치, 편향을 봐보자.\n",
    "import numpy as np\n",
    "x = np.array([   # 입력값\n",
    "    [1, 1],\n",
    "    [1, 0],\n",
    "    [0, 1],\n",
    "    [0, 0]\n",
    "])\n",
    "y = np.array([   # 기대출력\n",
    "    [1],\n",
    "    [0],\n",
    "    [0],\n",
    "    [0] \n",
    "])\n",
    "w = tf.random.normal([2], 0, 1)  # 입력값의 가중치\n",
    "                                 # 입력1, 입력2 -> 2개니까 가중치도 각각해서 총 2개\n",
    "b = tf.random.normal([1], 0, 1)  # 편향(bias)\n",
    "b_x = 1                          # 편향의 입력값\n",
    "alpha = 0.1                      # 학습률\n",
    "\n",
    "for i in range(2000):  # 학습수 : 2000번\n",
    "    error_sum = 0\n",
    "    for j in range(4): # 한 번 학습할 때마다 데이터 4개를 입력하니까\n",
    "        output =  sigmoid(np.sum(x[j] * w) + b_x * b)  # 입력값 * 가중치를 다 더함\n",
    "                                                       # 데이터 4개 + 편향 전체의 (입력값 * 가중치)의 합\n",
    "                                                       # 참고) numpy의 ndarray는 *할 때 shape에 맞게 알아서 조절이 된다.\n",
    "        error = y[j][0] - output  # 오차값 = 기대출력 - 출력\n",
    "        \n",
    "        # 경사하강법에 의한 가중치, 편향의 조정\n",
    "        w = w + x[j] * alpha * error\n",
    "        b = b + b_x * alpha * error\n",
    "        error_sum += error                             # 데이터별로 error 나오니까 데이터 4개의 에러총 합이 학습 1번할때마다의 오류\n",
    "\n",
    "print(\"w:\", w, \"b:\", b)\n",
    "\n",
    "# AND 신경망 네트워크의 가중치, 편향을 보면\n",
    "# 가중치1 : 6.95\n",
    "# 가중치2 : 6.95\n",
    "# 편향 : -10.61\n",
    "# 가중치1, 가중치2의 영향이 비슷하다. 편향의 영향도 크다.\n",
    "# 따라서 AND 신경망 네트워크는 기대출력과 가까운 출력값이 나온다는 것을 확인가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XOR, AND 신경망 네트워크의 가중치, 편향 비교해보자\n",
    "# AND 신경망 네트워크 : 어떤 일을 하려는지 명확하다!\n",
    "# - 가중치1, 가중치2 비슷하다.\n",
    "# - 입력1, 입력2 비슷한 중요도를 가진다.\n",
    "# - 편향값 : 큰 음수 > 중간계산값을 음수로 보내는 경향을 가진다.\n",
    "# - 가중치1, 가중치2를 합해야지 큰 음수값인 편향값에 준할 수 있다.\n",
    "\n",
    "# XOR 신경망 네트워크 : 어떤 일을 하려는지 명확하지 않다ㅠㅠ\n",
    "# - 중간계산값은 0에 가까워지고\n",
    "# - 출력값은 0.5에 가까워질뿐이다.\n",
    "\n",
    "# 즉, XOR 신경망 네트워크는 1개의 퍼셉트론으로 만들 수 없다는 것을 의미\n",
    "# 여러개의 퍼셉트론이 필요하다!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 퍼셉트론(Perceptron)=뉴런(Neuron)\n",
    "- <img src='https://blog.kakaocdn.net/dn/RTcpC/btqCqOo58fi/zKpF0Lm1lTjLq1xZnkCrl1/img.png' width=500>\n",
    "- 신경망(딥러닝)의 기원이 되는 알고리즘\n",
    "- 입력층과 출력층 만으로 구성되는 알고리즘\n",
    "- 다수의 신호 입력 -> 하나의 신호 출력\n",
    "- 출력 : 0 또는 1\n",
    "- 여러개의 퍼셉트론 사용 -> XOR 신경망 네트워크 등 어떠한 불린함수(Boolean function)이던지 풀 수 있다.\n",
    "    - Boolean function이란?\n",
    "    - 입력값 : 정수 -> 출력값 : 0 또는 1\n",
    "- 활성화 함수\n",
    "    - sigmoid : 출력값 0~1 사이\n",
    "    - ReLU : 출력값 0이상 > 왜곡 적음\n",
    "- 역전파\n",
    "    - 역방향으로 오차를 전파시키면서 각층의 가중치를 업데이트하고 최적의 학습 결과를 찾아가는 방법\n",
    "- 순전파 -> 오차발생 -> 역전파 -> 가중치 업데이트 -> 순전파.. 반복을 통해 오차값을 0에 가깝게 만든다.\n",
    "    \n",
    "\n",
    "- 단순(단층) 퍼셉트론\n",
    "    - 하나의 노드로 구성\n",
    "    - 중간층, 출력층의 구분이 없는 구조\n",
    "- 다층 퍼셉트론\n",
    "    - 여러개의 중간층으로 구성\n",
    "\n",
    "\n",
    "\n",
    "## 인공신경망(ANN : Artificial Neural Network)\n",
    "- ![인공신경망 그림](https://upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/300px-Colored_neural_network.svg.png)\n",
    " - 생물학의 신경망에서 영감을 얻은 통계학적 학습 알고리즘\n",
    " - 네트워크를 형성한 인공 뉴런이 학습을 통해 문제 해결 능력을 가지는 모델\n",
    " - 뉴런이 여러개 모여 층(layer)를 구성한 후, 이 층이 다시 모여 구성된 형태\n",
    " - 층\n",
    "     - 입력층\n",
    "     - 은닉층\n",
    "     - 출력층\n",
    "     \n",
    "     \n",
    "     \n",
    "## 딥러닝(Deep Learning)\n",
    "- 다층 인공신경망을 학습하는 알고리즘\n",
    "- 여러 비선형 변환기법의 조합을 통해 높은 수준의 추상화를 시도하는 머신러닝 알고리즘 집합\n",
    "    - 추상화?\n",
    "    - 다량의 데이터, 자료들 속에서 핵심적인 내용, 기능을 요약하는 작업\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras란?\n",
    "- python으로 작성된 오픈소스 신경망 라이브러리\n",
    "- Tensorflow 위에서 작동된다.\n",
    "\n",
    "\n",
    "- Tensorflow와 Keras의 차이점\n",
    "- Tensorflow : 디테일한 설정 가능, 내부 구조 확인할 수 있는 디버거 사용가능\n",
    "- Keras : 사용자 친화적, 간단한 조작만으로도 신경망 구성 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## keras sequential 모델\n",
    "- 참고 : https://keras.io/ko/getting-started/sequential-model-guide/\n",
    "- 참고 : https://www.codeonweb.com/entry/fe7882d2-e42a-4ef3-bbc2-e616d366e013\n",
    "- 레이어(layer, 층)을 선형으로 연결하여 구성한 것\n",
    "- 레이어 인스턴스를 생성자에 전달 -> sequential 모델 구성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## keras sequential 모델 만들기\n",
    "---\n",
    "### 입력 형태 지정하기\n",
    "- sequential 모델 1번째 계층에 입력 형태에 대한 정보 제공\n",
    "- 그 이후 계층은 자동으로 모양 추론됨\n",
    "    - input_shape : 형태정보, 튜플(정수 또는 None(임의의 양의 정수))\n",
    "    - Dense(32, input_shape=(784,))\n",
    "    - Dense(32, input_dim=784)    위의 두 식은 동일\n",
    "    - input_shape, input_dim, input_length\n",
    "    - batch_size : batch 데이터 크기\n",
    "    ```python\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense, Activation\n",
    "    \n",
    "    model = Sequential([               # 방법 1\n",
    "        Dense(32, input_shape=(784,)),\n",
    "        Activation(\"relu\"),\n",
    "        Dense(10),\n",
    "        Activation(\"softmax\"),\n",
    "        ])\n",
    "    \n",
    "    model = Sequential()                # 방법 2\n",
    "    model.add(Dense(32, input_dim=784))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    ```\n",
    "    \n",
    "    두 코드는 동일\n",
    "\n",
    "---\n",
    "### 컴파일\n",
    "- 모델 학습 전 학습 과정 구성\n",
    "- 최적화기(optimizer) : 정규화기\n",
    "    - 종류 : rmsprop, adagrad\n",
    "- 손실함수(loss function) : 모델이 최소화하려고 하는 대상, 기본제공되는 손실함수의 문자열 식별자 또는 목표함수 자체\n",
    "    - 종류 : categorical_crossentropy, mse\n",
    "- 메트릭(metric) : 기본제공되는 메트릭 또는 사용자정의 메트릭함수의 문자열 식별자\n",
    "    - 종류 : accuracy\n",
    "    ```python\n",
    "    # For a multi-class classification problem\n",
    "        model.compile(optimizer=\"rmsprop\",\n",
    "                      loss=\"categorical+corssentropy\",\n",
    "                      metrics=[\"accuracy\"])\n",
    "    \n",
    "    # For a binary classification problem\n",
    "        model.compile(optimizer=\"rmsprop\",\n",
    "                      loss=\"binary_corssentropy\",\n",
    "                      metrics=[\"accuracy\"])\n",
    "    \n",
    "    # For a mean squared error regression problem\n",
    "        model.compile(optimizer=\"rmsprop\",\n",
    "                      loss=\"mse\")\n",
    "    \n",
    "    # For custom metrics\n",
    "        import keras.backend as K\n",
    "        def mean_pred(y_true, y_pred):\n",
    "            return K.mean(y_pred)\n",
    "        model.compile(optimizer=\"rmsprop\",\n",
    "                      loss=\"binary_corssentropy\",\n",
    "                      metrics=[\"accuracy\", mean_pred])\n",
    "    ```\n",
    "\n",
    "---\n",
    "### 훈련\n",
    "- keras 모델은 Numpy 배열로 이루어진 입력 데이터, 레이블을 기반으로 훈련\n",
    "- 일반적으로 fit 함수 사용\n",
    "    ```python\n",
    "    model.fit(data, labels, epochs=10, batch_size=32)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이항분류\n",
    "- 데이터에 선 1개 그어서 분류\n",
    "- 주로 sigmoid 함수 사용\n",
    "\n",
    "\n",
    "$$\n",
    "s(z)=\\frac{1}{1+e^{-z}}\n",
    "$$\n",
    "<img src='https://t1.daumcdn.net/cfile/tistory/2312763759272E5B16' width=200>\n",
    "\n",
    "\n",
    "## 다항분류\n",
    "- 데이터에 선을 2개 이상 그어서 분류\n",
    "- softmax 함수 사용\n",
    "\n",
    "\n",
    "$$\n",
    "P_{i}(X_1,X_2)=\\frac{e^{f_{i}(x_1, x_2)}}{e^{f_{1}(x_1, x_2)}+e^{f_{2}(x_1, x_2)}+...+e^{f_{k}(x_1, x_2)}}\n",
    "$$\n",
    "\n",
    "\n",
    "<img src='https://t1.daumcdn.net/cfile/tistory/247E883D5927860832' width=400>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 2)                 6         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 9\n",
      "Trainable params: 9\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 3.27 tf.keras를 이용한 XOR 네트워크 계산\n",
    "import numpy as np\n",
    "x = np.array([\n",
    "    [1, 1],\n",
    "    [1, 0],\n",
    "    [0, 1],\n",
    "    [0, 0]\n",
    "])\n",
    "y = np.array([\n",
    "    [0],\n",
    "    [1],\n",
    "    [1],\n",
    "    [0]\n",
    "])\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(units=2, activation=\"sigmoid\", input_shape=(2,)),\n",
    "    tf.keras.layers.Dense(units=1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(lr=0.1), loss=\"mse\")\n",
    "\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
